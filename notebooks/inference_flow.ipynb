{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA_VISIBLE_DEVICES: None\n",
            "zsh:1: command not found: nvidia-smi\n",
            "PyTorch version: 2.2.1+cu121\n",
            "Is CUDA available: False\n",
            "CUDA version: N/A\n",
            "No GPU available\n",
            "Is CUDA initialized: False\n",
            "Using CPU tensors\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import wandb\n",
        "import torch\n",
        "\n",
        "# Step 1: Clear Environment Variables\n",
        "os.environ.pop('WANDB_API_KEY', None)\n",
        "\n",
        "# Step 2: Clear Wandb Config Directory\n",
        "wandb_config_dir = os.path.expanduser(\"~/.config/wandb\")\n",
        "if os.path.exists(wandb_config_dir):\n",
        "    shutil.rmtree(wandb_config_dir)\n",
        "\n",
        "import os\n",
        "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
        "\n",
        "# Try to force PyTorch to see the GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda if torch.cuda.is_available() else \"N/A\")\n",
        "\n",
        "# Print information about available GPUs\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU available\")\n",
        "\n",
        "# Check if CUDA is initialized\n",
        "print(\"Is CUDA initialized:\", torch.cuda.is_initialized())\n",
        "\n",
        "# If CUDA is available but not initialized, try to initialize it\n",
        "if torch.cuda.is_available() and not torch.cuda.is_initialized():\n",
        "    try:\n",
        "        torch.cuda.init()\n",
        "        print(\"CUDA initialized successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize CUDA: {e}\")\n",
        "\n",
        "# Set the default tensor type to cuda if available\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "    print(\"Default tensor type set to CUDA\")\n",
        "else:\n",
        "    print(\"Using CPU tensors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbbycYIegKO"
      },
      "source": [
        "# Operational"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFeDE4bhezNv"
      },
      "outputs": [],
      "source": [
        "#Load the numpy arrays\n",
        "bi_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/bi_np.pkl')\n",
        "erc_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/erc_np.pkl')\n",
        "pyrome_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/pyrome_np.pkl')\n",
        "wui_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/wui_np.pkl')\n",
        "\n",
        "#Load the latitude and longitude matrices\n",
        "lats_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/lats_np.pkl', nodata_value=None)\n",
        "longs_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/longs_np.pkl', nodata_value=None)\n",
        "\n",
        "print(\"Shapes of the loaded numpy arrays:\")\n",
        "print(f\"BI: {bi_np_loaded.shape}\")\n",
        "print(f\"ERC: {erc_np_loaded.shape}\")\n",
        "print(f\"Pyrome: {pyrome_np_loaded.shape}\")\n",
        "print(f\"WUI: {wui_np_loaded.shape}\")\n",
        "print(\"Shapes of the latitude and longitude matrices:\")\n",
        "print(f\"Lats: {lats_np_loaded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7XmbJ0ze6Mi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J6dJUp1PejEZ"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "from dask import delayed, compute\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Function to load shapefile and convert CRS\n",
        "def load_shapefile(shapefile_path):\n",
        "    pyromes_gdf = gpd.read_file(shapefile_path)\n",
        "    pyromes_gdf = pyromes_gdf.to_crs(epsg=4326)\n",
        "    return pyromes_gdf\n",
        "\n",
        "# Load the shapefile\n",
        "shapefile_path = '/content/drive/My Drive/CloudFire/data/pyrome_shp/Pyromes_CONUS_20200206.shp'\n",
        "pyromes_gdf = load_shapefile(shapefile_path)\n",
        "\n",
        "# Define the shape and ignition date\n",
        "ignition_date = '2023-08-01'\n",
        "shape = bi_np_loaded.shape\n",
        "\n",
        "# Function to process each pixel\n",
        "@delayed\n",
        "def process_pixel(i, j):\n",
        "    if np.isnan(bi_np_loaded[i, j]):\n",
        "        return None\n",
        "\n",
        "    pyrome = pyrome_np_loaded[i, j]\n",
        "    pyrome_geom = pyromes_gdf[pyromes_gdf['PYROME'] == pyrome].geometry\n",
        "    if pyrome_geom.empty:\n",
        "        return None\n",
        "\n",
        "    random_point = pyrome_geom.sample(1).centroid.iloc[0]\n",
        "\n",
        "    return {\n",
        "        'Ignition date': ignition_date,\n",
        "        'Ignition time': np.nan,\n",
        "        'Containment date': np.nan,\n",
        "        'Containment time': np.nan,\n",
        "        'State': np.nan,\n",
        "        'Name': np.nan,\n",
        "        'MTBS ID': np.nan,\n",
        "        'Longitude': random_point.x,\n",
        "        'Latitude': random_point.y,\n",
        "        'Acres': 0,\n",
        "        'BI': bi_np_loaded[i, j],\n",
        "        'ERC': erc_np_loaded[i, j],\n",
        "        'WUI proximity': wui_np_loaded[i, j],\n",
        "        'Pyrome': pyrome,\n",
        "        'X pixel': i,\n",
        "        'Y pixel': j\n",
        "    }\n",
        "\n",
        "# Create a list of delayed tasks\n",
        "tasks = [process_pixel(i, j) for i in range(shape[0]) for j in range(shape[1])]\n",
        "\n",
        "# Compute the tasks in parallel\n",
        "results = compute(*tasks)\n",
        "\n",
        "# Filter out None results\n",
        "results = [res for res in results if res is not None]\n",
        "\n",
        "# Convert to a DataFrame\n",
        "op_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(op_df.head())\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "# op_df.to_csv('wildfire_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zWRPj5Nh45N"
      },
      "outputs": [],
      "source": [
        "op_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tIHtiZzfv4Y"
      },
      "outputs": [],
      "source": [
        "proc_op_df = preprocess_data(op_df, fit=False)\n",
        "proc_op_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p3lVmPNfQX6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming model, aug_proc_df, and var_range are already defined and available\n",
        "\n",
        "# Convert the augmented data to tensor\n",
        "augmented_data_tensor = torch.tensor(proc_op_df.drop(columns=['Acres']).values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Generate a range of potential wildfire sizes (log-scaled)\n",
        "fire_sizes_log = torch.linspace(0, 10, steps=40).unsqueeze(1).to(device)\n",
        "\n",
        "# Threshold for large fires (log-scaled)\n",
        "large_fire_threshold_log = np.log1p(5000)\n",
        "\n",
        "# Run inference\n",
        "model.eval()\n",
        "pdf_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(augmented_data_tensor.size(0)):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing row {i+1}/{augmented_data_tensor.size(0)}\")\n",
        "        context_point = augmented_data_tensor[i].unsqueeze(0).repeat(fire_sizes_log.size(0), 1)\n",
        "        log_probs = model.log_prob(fire_sizes_log, context=context_point)\n",
        "        pdf_values.append(torch.exp(log_probs.cpu()).numpy())\n",
        "\n",
        "# Reshape the PDF values to match the grid\n",
        "#pdf_values = np.array(pdf_values).reshape(len(var_range), len(var_range), -1)\n",
        "pdf_values = np.array(pdf_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJbSGyL-hjxr"
      },
      "outputs": [],
      "source": [
        "pdf_values = np.array(pdf_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1t3tLf_g62p"
      },
      "outputs": [],
      "source": [
        "# Check the shape of pdf_values and op_df to ensure they match in the number of rows\n",
        "assert pdf_values.shape[0] == len(op_df), \"The number of rows in pdf_values must match the number of rows in op_df\"\n",
        "\n",
        "# Create new column names for each threshold\n",
        "threshold_columns = [f'thres_{i}' for i in range(pdf_values.shape[1])]\n",
        "\n",
        "# Add the columns to op_df\n",
        "for idx, col_name in enumerate(threshold_columns):\n",
        "    op_df[col_name] = pdf_values[:, idx]\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9PuGc8MtVbU"
      },
      "outputs": [],
      "source": [
        "print(op_df.columns)\n",
        "op_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KilUOp_gufBu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming op_df and pdf_values are already defined\n",
        "\n",
        "# Define the shape of the image\n",
        "image_shape = (36, 46)\n",
        "\n",
        "# Create the figure and axes for 40 subplots\n",
        "fig, axes = plt.subplots(7, 9, figsize=(20, 15), constrained_layout=True)\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Define thresholds\n",
        "thresholds = np.exp(np.linspace(0, 10, 40)) - 1  # Transform back from log scale\n",
        "\n",
        "# Loop over each threshold and plot the corresponding heatmap\n",
        "for i, ax in enumerate(axes)[:-1]:\n",
        "    # Create an empty grid with np.nan\n",
        "    grid = np.full(image_shape, np.nan)\n",
        "\n",
        "    # Fill the grid with the pdf values\n",
        "    for idx, row in op_df.iterrows():\n",
        "        x = int(row['X pixel'])\n",
        "        y = int(row['Y pixel'])\n",
        "        grid[x, y] = row[f'thres_{i}']\n",
        "\n",
        "    # Plot the heatmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=\"viridis\", cbar=i == 0, vmin=0, vmax=1)\n",
        "\n",
        "    # Set the title with the threshold value\n",
        "    ax.set_title(f'Threshold: {thresholds[i]:.2f} Acres')\n",
        "\n",
        "    # Remove x and y ticks for clarity\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Add a single color bar for the entire figure\n",
        "fig.colorbar(axes[0].collections[0], ax=axes, orientation='horizontal', fraction=0.02, pad=0.04)\n",
        "\n",
        "# Show the plot\n",
        "plt.suptitle('Probability of Wildfires Greater Than Threshold Acres', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioHPyy7LwGI0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming op_df and pdf_values are already defined\n",
        "\n",
        "# Define the shape of the image\n",
        "image_shape = (36, 46)\n",
        "\n",
        "# Define thresholds and start at index 18\n",
        "thresholds = np.exp(np.linspace(0, 10, 40)) - 1  # Transform back from log scale\n",
        "start_idx = 18\n",
        "thresholds = thresholds[start_idx:]\n",
        "\n",
        "# Define color palette\n",
        "colors = sns.color_palette(\"husl\", len(thresholds))\n",
        "\n",
        "# Number of columns and rows\n",
        "ncols = 4\n",
        "nrows = int(np.ceil(len(thresholds) / ncols))\n",
        "\n",
        "# Create the figure and axes for the subplots\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 5 * nrows), constrained_layout=True)\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop over each threshold and plot the corresponding heatmap\n",
        "for i, ax in enumerate(axes):\n",
        "    if i >= len(thresholds):\n",
        "        ax.axis('off')  # Hide unused subplots\n",
        "        continue\n",
        "\n",
        "    threshold = thresholds[i]\n",
        "    # Create an empty grid with np.nan\n",
        "    grid = np.full(image_shape, np.nan)\n",
        "\n",
        "    # Fill the grid with the pdf values\n",
        "    for idx, row in op_df.iterrows():\n",
        "        x = int(row['X pixel'])\n",
        "        y = int(row['Y pixel'])\n",
        "        grid[x, y] = row[f'thres_{i + start_idx}']\n",
        "\n",
        "    # Calculate min, max, and average\n",
        "    valid_values = grid[~np.isnan(grid)]\n",
        "    min_val = np.min(valid_values)\n",
        "    max_val = np.max(valid_values)\n",
        "    avg_val = np.mean(valid_values)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    sns.heatmap(grid, ax=ax, cmap=sns.color_palette(\"coolwarm\", as_cmap=True), cbar=i == 0, vmin=0, vmax=1)\n",
        "\n",
        "    # Set the title with the threshold value, min, max, and average\n",
        "    ax.set_title(f'Threshold: {threshold:.2f} Acres\\nMin: {min_val:.2f}, Max: {max_val:.2f}, Avg: {avg_val:.2f}')\n",
        "\n",
        "    # Remove x and y ticks for clarity\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Add a single color bar for the entire figure\n",
        "fig.colorbar(axes[0].collections[0], ax=axes, orientation='horizontal', fraction=0.02, pad=0.04)\n",
        "\n",
        "# Show the plot\n",
        "plt.suptitle('Probability of Wildfires Greater Than Threshold Acres', fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lrl1RGdl2-sD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming op_df and pdf_values are already defined\n",
        "\n",
        "# Define the shape of the image\n",
        "image_shape = (36, 46)\n",
        "\n",
        "# Calculate the 99th percentile fire size for each pixel\n",
        "fire_sizes_99th = np.empty(image_shape)\n",
        "fire_sizes_99th.fill(np.nan)\n",
        "\n",
        "for idx, row in op_df.iterrows():\n",
        "    x = int(row['X pixel'])\n",
        "    y = int(row['Y pixel'])\n",
        "    pdf_vals = row[[f'thres_{i}' for i in range(40)]].values\n",
        "    cdf_vals = np.cumsum(pdf_vals)\n",
        "    cdf_vals /= cdf_vals[-1]  # Normalize to [0, 1]\n",
        "    fire_size_index = np.searchsorted(cdf_vals, 0.99)\n",
        "    fire_sizes_99th[x, y] = np.exp(np.linspace(0, 10, 40)[fire_size_index]) - 1  # Transform back from log scale\n",
        "\n",
        "# Plot the heatmap of the 99th percentile fire sizes\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.heatmap(fire_sizes_99th, cmap='viridis', cbar_kws={'label': '99th Percentile Fire Size (Acres)'}, vmin=0)\n",
        "plt.title('99th Percentile Fire Size for Each Pixel')\n",
        "plt.xlabel('X pixel')\n",
        "plt.ylabel('Y pixel')\n",
        "plt.show()\n",
        "\n",
        "# Create a DataFrame for 99th percentile fire sizes per pyrome\n",
        "pyrome_99th_fire_sizes = []\n",
        "\n",
        "for pyrome in op_df['Pyrome'].unique():\n",
        "    pyrome_data = op_df[op_df['Pyrome'] == pyrome]\n",
        "    pyrome_fire_sizes = []\n",
        "    for idx, row in pyrome_data.iterrows():\n",
        "        x = int(row['X pixel'])\n",
        "        y = int(row['Y pixel'])\n",
        "        fire_size = fire_sizes_99th[x, y]\n",
        "        if not np.isnan(fire_size):\n",
        "            pyrome_fire_sizes.append(fire_size)\n",
        "    if pyrome_fire_sizes:\n",
        "        pyrome_99th_fire_sizes.append((pyrome, np.mean(pyrome_fire_sizes)))\n",
        "\n",
        "pyrome_99th_fire_sizes_df = pd.DataFrame(pyrome_99th_fire_sizes, columns=['Pyrome', '99th Percentile Fire Size'])\n",
        "\n",
        "# Sort by 99th percentile fire size for better visualization\n",
        "pyrome_99th_fire_sizes_df = pyrome_99th_fire_sizes_df.sort_values(by='99th Percentile Fire Size', ascending=False)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.barplot(data=pyrome_99th_fire_sizes_df, x='Pyrome', y='99th Percentile Fire Size', palette='viridis')\n",
        "plt.title('99th Percentile Fire Size per Pyrome')\n",
        "plt.xlabel('Pyrome')\n",
        "plt.ylabel('99th Percentile Fire Size (Acres)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5mJrv_7vz94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming op_df and pdf_values are already defined\n",
        "\n",
        "# Convert the DataFrame to numeric values for percentile calculations\n",
        "op_df_numeric = op_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Calculate the 99th percentile for each pyrome\n",
        "percentiles = {}\n",
        "for pyrome in op_df['Pyrome'].unique():\n",
        "    pyrome_data = op_df_numeric[op_df['Pyrome'] == pyrome]\n",
        "    # Stack the threshold columns and compute the 99th percentile\n",
        "    thres_values = pyrome_data.filter(like='thres_').values.flatten()\n",
        "    percentiles[pyrome] = np.percentile(thres_values, 99)\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "percentiles_df = pd.DataFrame(list(percentiles.items()), columns=['Pyrome', '99th Percentile'])\n",
        "\n",
        "# Sort by 99th percentile value for better visualization\n",
        "percentiles_df = percentiles_df.sort_values(by='99th Percentile', ascending=False)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.barplot(data=percentiles_df, x='Pyrome', y='99th Percentile', palette='viridis')\n",
        "plt.title('99th Percentile Max Fire Size per Pyrome')\n",
        "plt.xlabel('Pyrome')\n",
        "plt.ylabel('99th Percentile Max Fire Size (Probability)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwh3i-W8u6Yf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Fm5z9ZB5A5"
      },
      "source": [
        "# GPD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lPSWDDmB6nX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Ignition date'] = pd.to_datetime(df['Ignition date'])\n",
        "df['Containment date'] = pd.to_datetime(df['Containment date'])\n",
        "\n",
        "# Calculate fire duration\n",
        "df['Duration'] = (df['Containment date'] - df['Ignition date']).dt.days\n",
        "df = df[['Latitude', 'Longitude', 'ERC', 'WUI proximity', 'BI', 'Pyrome', 'Acres', 'Ignition date']]\n",
        "\n",
        "# Filter out the dataset to include only the Western United States\n",
        "# Filter out if not in np.unique(pyrome_np_loaded)\n",
        "df = df[df['Pyrome'].isin(np.unique(pyrome_np_loaded))]\n",
        "#df = df[df['Pyrome'] < 60]\n",
        "# Handle missing values (example: fill with mean)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#Drop Rows with NaN values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the shape of the filtered dataset to confirm the filtering\n",
        "print(\"Filtered dataset shape:\", df.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Convert ERC, WUI, and BI to percentiles within each pyrome\n",
        "def convert_to_percentile(df, column, group_by_column):\n",
        "    df[f'{column}_percentile'] = df.groupby(group_by_column)[column].rank(pct=True) * 100\n",
        "    return df\n",
        "\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df = convert_to_percentile(df, col, 'Pyrome')\n",
        "\n",
        "# One-hot encode the pyrome column\n",
        "encoder = OneHotEncoder()\n",
        "pyrome_encoded = encoder.fit_transform(df[['Pyrome']]).toarray()\n",
        "pyrome_df = pd.DataFrame(pyrome_encoded, columns=encoder.get_feature_names_out(['Pyrome']))\n",
        "\n",
        "# Compute the ignition date as cyclically encoded variables\n",
        "def date_sin(date):\n",
        "    return np.sin(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "def date_cos(date):\n",
        "    return np.cos(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "df['ignition_sin'] = df['Ignition date'].apply(date_sin)\n",
        "df['ignition_cos'] = df['Ignition date'].apply(date_cos)\n",
        "\n",
        "# Combine all features into a single dataframe\n",
        "model_df = pd.concat([df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres', 'ignition_sin', 'ignition_cos']], pyrome_df], axis=1)\n",
        "\n",
        "# Standardize continuous variables\n",
        "scaler = StandardScaler()\n",
        "continuous_vars = model_df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos']]\n",
        "scaled_vars = scaler.fit_transform(continuous_vars)\n",
        "scaled_df = pd.DataFrame(scaled_vars, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "model_df = pd.concat([scaled_df, model_df[encoder.get_feature_names_out(['Pyrome'])], model_df['Acres']], axis=1)\n",
        "\n",
        "# Filter out records with Acres <= 1000\n",
        "model_df = model_df[model_df['Acres'] > 1000]\n",
        "\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n",
        "\n",
        "model_df.fillna(model_df.mean(), inplace=True)\n",
        "\n",
        "print(\"Filtered model_df shape:\", model_df.shape)\n",
        "\n",
        "# Check for NaN values in model_df and print them if they exist\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = model_df.drop(columns=['Acres'])\n",
        "y = model_df['Acres']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgJp1saBHPTJ"
      },
      "outputs": [],
      "source": [
        "model_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyiknvg4KV6b"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "# Check for NaN or infinite values in X and y\n",
        "print(\"NaN in X:\", np.any(np.isnan(X)))\n",
        "print(\"Infinite in X:\", np.any(np.isinf(X)))\n",
        "print(\"NaN in y:\", np.any(np.isnan(y)))\n",
        "print(\"Infinite in y:\", np.any(np.isinf(y)))\n",
        "\n",
        "# Replace NaN or infinite values if any\n",
        "X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "y = np.nan_to_num(y, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Define custom GPD loss function for XGBoost\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define the GPD loss function for XGBoost\n",
        "def gpd_loss(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    threshold = 2000  # example threshold\n",
        "    excess = labels - threshold\n",
        "\n",
        "    # Mask to identify values above the threshold\n",
        "    mask = labels > threshold\n",
        "\n",
        "    # Example shape parameter (could also be learned)\n",
        "    xi = 0.1\n",
        "    sigma = np.exp(preds)  # scale parameter from the model predictions\n",
        "\n",
        "    # Apply mask to excess and sigma\n",
        "    excess = excess[mask]\n",
        "    sigma = sigma[mask]\n",
        "\n",
        "    # Gradient calculation\n",
        "    grad = -((1 + xi * (excess / sigma))**(-1/xi) * (excess / sigma + 1 / xi) / sigma)\n",
        "\n",
        "    # Hessian calculation\n",
        "    hess = grad * (grad / sigma + (1 + xi * (excess / sigma))**(-1/xi - 1) / sigma)\n",
        "\n",
        "    # Initialize full gradient and hessian arrays with zeros\n",
        "    full_grad = np.zeros_like(labels)\n",
        "    full_hess = np.zeros_like(labels)\n",
        "\n",
        "    # Set gradient and hessian for values above the threshold\n",
        "    full_grad[mask] = grad\n",
        "    full_hess[mask] = hess\n",
        "\n",
        "    return full_grad, full_hess\n",
        "\n",
        "# Load and preprocess your data here...\n",
        "\n",
        "# Train XGBoost model with custom GPD loss function\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # using squared error as a baseline\n",
        "    'eval_metric': 'rmse',\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6\n",
        "}\n",
        "\n",
        "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "evals_result = {}\n",
        "\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100, evals=evals, evals_result=evals_result, obj=gpd_loss, verbose_eval=True)\n",
        "\n",
        "# Print training history\n",
        "print(evals_result)\n",
        "\n",
        "# Plot training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = len(evals_result['train']['rmse'])\n",
        "x_axis = range(0, epochs)\n",
        "\n",
        "# Plot training and validation RMSE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_axis, evals_result['train']['rmse'], label='Train')\n",
        "plt.plot(x_axis, evals_result['eval']['rmse'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('XGBoost GPD Loss Training and Validation RMSE')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErbstsrtLZS-"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPYU6nrfCqnX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "# Check for NaN or infinite values in X and y\n",
        "print(\"NaN in X:\", np.any(np.isnan(X)))\n",
        "print(\"Infinite in X:\", np.any(np.isinf(X)))\n",
        "print(\"NaN in y:\", np.any(np.isnan(y)))\n",
        "print(\"Infinite in y:\", np.any(np.isinf(y)))\n",
        "\n",
        "# Replace NaN or infinite values if any\n",
        "X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "y = np.nan_to_num(y, nan=0.0, posinf=1e10, neginf=-1e10)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the GPD loss function\n",
        "def gpd_loss(y_true, y_pred):\n",
        "    xi = 0.1  # Example shape parameter\n",
        "    kappa = 0.95  # Example threshold parameter\n",
        "    epsilon = 1e-6  # Small value to ensure numerical stability\n",
        "\n",
        "    # Calculate the log scale parameter\n",
        "    sigma = K.exp(y_pred) + epsilon  # Adding epsilon to avoid log(0)\n",
        "\n",
        "    # Compute the GPD loss\n",
        "    term1 = (xi + 1) / xi * K.log(1 + (y_true * ((1 - kappa)**(-xi) - 1)) / sigma + epsilon)\n",
        "    term2 = K.log(xi * sigma / ((1 - kappa)**xi - 1) + epsilon)\n",
        "\n",
        "    return K.mean(term1 + term2)\n",
        "\n",
        "# Define the neural network architecture\n",
        "def create_model(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))  # Output layer for the log scale parameter\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X_train.shape[1]\n",
        "model = create_model(input_dim)\n",
        "\n",
        "# Compile the model with the custom GPD loss\n",
        "model.compile(optimizer='adam', loss=gpd_loss)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Plot training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZXQ1sw0GMIYf"
      },
      "outputs": [],
      "source": [
        "!pip install elapid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gqGF52pMNiA"
      },
      "outputs": [],
      "source": [
        "!pip install stemflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RGNQw2WMDqu"
      },
      "outputs": [],
      "source": [
        "import elapid as ela\n",
        "from stemflow.model.AdaSTEM import AdaSTEM, AdaSTEMRegressor\n",
        "from stemflow.model_selection import ST_CV, ST_train_test_split\n",
        "from stemflow.utils.plot_gif import make_sample_gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP-HTfccCXFY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Ignition date'] = pd.to_datetime(df['Ignition date'])\n",
        "df['Containment date'] = pd.to_datetime(df['Containment date'])\n",
        "\n",
        "# Calculate fire duration\n",
        "df['Duration'] = (df['Containment date'] - df['Ignition date']).dt.days\n",
        "df = df[['Latitude', 'Longitude', 'ERC', 'WUI proximity', 'BI', 'Pyrome', 'Acres', 'Ignition date']]\n",
        "\n",
        "# Filter out the dataset to include only the Western United States\n",
        "# Filter out if not in np.unique(pyrome_np_loaded)\n",
        "df = df[df['Pyrome'].isin(np.unique(pyrome_np_loaded))]\n",
        "#df = df[df['Pyrome'] < 60]\n",
        "# Handle missing values (example: fill with mean)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#Drop Rows with NaN values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the shape of the filtered dataset to confirm the filtering\n",
        "print(\"Filtered dataset shape:\", df.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Convert ERC, WUI, and BI to percentiles within each pyrome\n",
        "def convert_to_percentile(df, column, group_by_column):\n",
        "    df[f'{column}_percentile'] = df.groupby(group_by_column)[column].rank(pct=True) * 100\n",
        "    return df\n",
        "\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df = convert_to_percentile(df, col, 'Pyrome')\n",
        "\n",
        "# One-hot encode the pyrome column\n",
        "encoder = OneHotEncoder()\n",
        "pyrome_encoded = encoder.fit_transform(df[['Pyrome']]).toarray()\n",
        "pyrome_df = pd.DataFrame(pyrome_encoded, columns=encoder.get_feature_names_out(['Pyrome']))\n",
        "\n",
        "# Compute the ignition date as cyclically encoded variables\n",
        "def date_sin(date):\n",
        "    return np.sin(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "def date_cos(date):\n",
        "    return np.cos(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "df['ignition_sin'] = df['Ignition date'].apply(date_sin)\n",
        "df['ignition_cos'] = df['Ignition date'].apply(date_cos)\n",
        "\n",
        "\n",
        "# Combine all features into a single dataframe\n",
        "model_df = pd.concat([df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres', 'ignition_sin', 'ignition_cos']], pyrome_df], axis=1)\n",
        "\n",
        "# Standardize continuous variables\n",
        "scaler = StandardScaler()\n",
        "continuous_vars = model_df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos']]\n",
        "scaled_vars = scaler.fit_transform(continuous_vars)\n",
        "scaled_df = pd.DataFrame(scaled_vars, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "model_df = pd.concat([scaled_df, model_df[encoder.get_feature_names_out(['Pyrome'])], model_df['Acres']], axis=1)\n",
        "\n",
        "# Filter out records with Acres <= 1000\n",
        "model_df = model_df[model_df['Acres'] > 1000]\n",
        "\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n",
        "\n",
        "model_df.fillna(model_df.mean(), inplace=True)\n",
        "\n",
        "print(\"Filtered model_df shape:\", model_df.shape)\n",
        "\n",
        "# Check for NaN values in model_df and print them if they exist\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = model_df.drop(columns=['Acres'])\n",
        "y = model_df['Acres']\n",
        "model_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMa_5cv0WHIY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsGv1ecrDKa-"
      },
      "source": [
        "# Extreme Value Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhqCxLFJHhWT"
      },
      "outputs": [],
      "source": [
        "# Fit and plot GPD for each Pyrome\n",
        "pyrome_columns = [col for col in model_df.columns if 'Pyrome_' in col]\n",
        "\n",
        "# Create subplots\n",
        "n_cols = 3\n",
        "n_rows = int(np.ceil(len(pyrome_columns) / n_cols))\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "\n",
        "for idx, pyrome_col in enumerate(pyrome_columns):\n",
        "    # Filter data for the current Pyrome\n",
        "    pyrome_data = model_df[model_df[pyrome_col] == 1]\n",
        "    if pyrome_data.shape[0] > 0:\n",
        "        y_pyrome = pyrome_data['Acres']\n",
        "\n",
        "        # Fit GPD\n",
        "        shape, loc, scale = stats.genpareto.fit(y_pyrome)\n",
        "\n",
        "        # Generate values from the fitted GPD for the histogram range\n",
        "        x = np.linspace(min(y_pyrome), max(y_pyrome), 1000)\n",
        "        gpd_pdf = stats.genpareto.pdf(x, shape, loc, scale) * len(y_pyrome) * (max(y_pyrome) - min(y_pyrome)) / 50  # scale to match the histogram\n",
        "\n",
        "        # Goodness of fit metric (Kolmogorov-Smirnov statistic)\n",
        "        ks_statistic, ks_p_value = stats.kstest(y_pyrome, 'genpareto', args=(shape, loc, scale))\n",
        "\n",
        "        # Plot histogram of wildfire sizes with GPD overlay\n",
        "        ax = axes[idx // n_cols, idx % n_cols]\n",
        "        ax.hist(y_pyrome, bins=50, edgecolor='k', alpha=0.7, label='Wildfire Sizes')\n",
        "        ax.plot(x, gpd_pdf, 'r-', lw=2, label='Fitted GPD')\n",
        "        ax.set_xlabel('Wildfire Size (Acres)')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        #log scale\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_title(f'{pyrome_col}\\nSamples: {len(y_pyrome)}, Shape: {shape:.2f}, Loc: {loc:.2f}, Scale: {scale:.2f}\\nKS Statistic: {ks_statistic:.2f}, p-value: {ks_p_value:.2f}')\n",
        "        ax.legend()\n",
        "\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqSj5lSGGU_I"
      },
      "outputs": [],
      "source": [
        "# Fit a Generalized Pareto Distribution to the data\n",
        "shape, loc, scale = stats.genpareto.fit(y)\n",
        "\n",
        "# Generate values from the fitted GPD for the histogram range\n",
        "x = np.linspace(min(y), max(y), 1000)\n",
        "gpd_pdf = stats.genpareto.pdf(x, shape, loc, scale) * len(y) * (max(y) - min(y)) / 50  # scale to match the histogram\n",
        "\n",
        "# Plot histogram of wildfire sizes with GPD overlay\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y, bins=50, edgecolor='k', alpha=0.7, label='Wildfire Sizes')\n",
        "plt.plot(x, gpd_pdf, 'r-', lw=2, label='Fitted GPD')\n",
        "plt.xlabel('Wildfire Size (Acres)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Wildfire Sizes with Fitted GPD')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag4Ko0oDDTOc"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot 1: Histogram of Wildfire Sizes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y, bins=50, edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Wildfire Size (Acres)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Wildfire Sizes')\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Q-Q Plot\n",
        "shape, loc, scale = stats.genpareto.fit(y)\n",
        "quantiles = np.linspace(0, 1, len(y))\n",
        "sorted_data = np.sort(y)\n",
        "gpd_quantiles = stats.genpareto.ppf(quantiles, shape, loc, scale)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(gpd_quantiles, sorted_data, 'o')\n",
        "plt.plot([min(gpd_quantiles), max(gpd_quantiles)], [min(gpd_quantiles), max(gpd_quantiles)], 'r--')\n",
        "plt.xlabel('Theoretical Quantiles (GPD)')\n",
        "plt.ylabel('Sample Quantiles')\n",
        "plt.title('Q-Q Plot of Wildfire Sizes vs GPD')\n",
        "plt.show()\n",
        "\n",
        "def mean_excess_function(data, thresholds):\n",
        "    excess_means = []\n",
        "    for threshold in thresholds:\n",
        "        excesses = data[data > threshold] - threshold\n",
        "        excess_means.append(np.mean(excesses))\n",
        "    return excess_means\n",
        "\n",
        "thresholds = np.linspace(min(y), max(y), 100)\n",
        "mean_excess = mean_excess_function(y, thresholds)\n",
        "\n",
        "# Plot 3: Mean Excess Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, mean_excess, 'b-')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Mean Excess')\n",
        "plt.title('Mean Excess Plot')\n",
        "plt.show()\n",
        "\n",
        "def hill_estimator(data, k):\n",
        "    sorted_data = np.sort(data)\n",
        "    n = len(sorted_data)\n",
        "    hill_estimates = []\n",
        "    for i in range(1, k + 1):\n",
        "        hill_estimates.append(np.mean(np.log(sorted_data[n - i:n]) - np.log(sorted_data[n - i - 1])))\n",
        "    return hill_estimates\n",
        "\n",
        "k_values = range(1, len(y) // 2)\n",
        "hill_estimates = hill_estimator(y, max(k_values))\n",
        "\n",
        "# Plot 4: Hill Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, hill_estimates, 'g-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Hill Estimate')\n",
        "plt.title('Hill Plot')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhH5qB_xJxw6"
      },
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMLq6zBzJzbC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Ignition date'] = pd.to_datetime(df['Ignition date'])\n",
        "df['Containment date'] = pd.to_datetime(df['Containment date'])\n",
        "\n",
        "# Calculate fire duration\n",
        "df['Duration'] = (df['Containment date'] - df['Ignition date']).dt.days\n",
        "df = df[['Latitude', 'Longitude', 'ERC', 'WUI proximity', 'BI', 'Pyrome', 'Acres', 'Ignition date']]\n",
        "\n",
        "# Filter out the dataset to include only the Western United States\n",
        "# Filter out if not in np.unique(pyrome_np_loaded)\n",
        "df = df[df['Pyrome'].isin(np.unique(pyrome_np_loaded))]\n",
        "#df = df[df['Pyrome'] < 60]\n",
        "# Handle missing values (example: fill with mean)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#Drop Rows with NaN values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the shape of the filtered dataset to confirm the filtering\n",
        "print(\"Filtered dataset shape:\", df.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Convert ERC, WUI, and BI to percentiles within each pyrome\n",
        "def convert_to_percentile(df, column, group_by_column):\n",
        "    df[f'{column}_percentile'] = df.groupby(group_by_column)[column].rank(pct=True) * 100\n",
        "    return df\n",
        "\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df = convert_to_percentile(df, col, 'Pyrome')\n",
        "\n",
        "# One-hot encode the pyrome column\n",
        "encoder = OneHotEncoder()\n",
        "pyrome_encoded = encoder.fit_transform(df[['Pyrome']]).toarray()\n",
        "pyrome_df = pd.DataFrame(pyrome_encoded, columns=encoder.get_feature_names_out(['Pyrome']))\n",
        "\n",
        "# Compute the ignition date as cyclically encoded variables\n",
        "def date_sin(date):\n",
        "    return np.sin(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "def date_cos(date):\n",
        "    return np.cos(2 * np.pi * date.timetuple().tm_yday / 366.)\n",
        "\n",
        "df['ignition_sin'] = df['Ignition date'].apply(date_sin)\n",
        "df['ignition_cos'] = df['Ignition date'].apply(date_cos)\n",
        "\n",
        "# Combine all features into a single dataframe\n",
        "model_df = pd.concat([df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres', 'ignition_sin', 'ignition_cos']], pyrome_df], axis=1)\n",
        "\n",
        "# Standardize continuous variables\n",
        "scaler = StandardScaler()\n",
        "continuous_vars = model_df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos']]\n",
        "scaled_vars = scaler.fit_transform(continuous_vars)\n",
        "scaled_df = pd.DataFrame(scaled_vars, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "model_df = pd.concat([scaled_df, model_df[encoder.get_feature_names_out(['Pyrome'])], model_df['Acres']], axis=1)\n",
        "\n",
        "# Filter out records with Acres <= 1000\n",
        "model_df = model_df[model_df['Acres'] > 100]\n",
        "\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n",
        "\n",
        "model_df.fillna(model_df.mean(), inplace=True)\n",
        "\n",
        "print(\"Filtered model_df shape:\", model_df.shape)\n",
        "\n",
        "# Check for NaN values in model_df and print them if they exist\n",
        "nan_values = model_df.isna().sum()\n",
        "print(\"NaN values in model_df:\\n\", nan_values[nan_values > 0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea4Dk57FLgXX"
      },
      "outputs": [],
      "source": [
        "model_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJh0eiNPLc_N"
      },
      "outputs": [],
      "source": [
        "model_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o_xYNYJLOlK"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "# Function to select the optimal number of components using BIC\n",
        "def select_optimal_gmm(X, max_components=10):\n",
        "    bic_scores = []\n",
        "    models = []\n",
        "\n",
        "    for n_components in range(1, max_components + 1):\n",
        "        model = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "        model.fit(X)\n",
        "        bic = model.bic(X)\n",
        "        bic_scores.append(bic)\n",
        "        models.append(model)\n",
        "\n",
        "    optimal_index = np.argmin(bic_scores)\n",
        "    optimal_model = models[optimal_index]\n",
        "\n",
        "    return optimal_model, bic_scores, optimal_index + 1\n",
        "\n",
        "# Extract features and target variable\n",
        "X = model_df.drop(columns=['Acres']).values\n",
        "y = model_df['Acres'].values\n",
        "\n",
        "# Select the optimal number of components\n",
        "optimal_gmm, bic_scores, optimal_n_components = select_optimal_gmm(X)\n",
        "\n",
        "print(f\"Optimal number of components: {optimal_n_components}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DETqYa-fLt9k"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot BIC scores\n",
        "def plot_bic_scores(bic_scores):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(bic_scores) + 1), bic_scores, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('BIC Score')\n",
        "    plt.title('BIC Scores for Different Number of Components')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the BIC scores\n",
        "plot_bic_scores(bic_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH30MIhZL4yc"
      },
      "outputs": [],
      "source": [
        "# Fit the GMM with the optimal number of components\n",
        "optimal_gmm.fit(X)\n",
        "\n",
        "# Predict the component probabilities for each sample\n",
        "probs = optimal_gmm.predict_proba(X)\n",
        "\n",
        "# Predict the component means for each sample\n",
        "predictions = optimal_gmm.predict(X)\n",
        "\n",
        "# Append the probabilities and predictions to the dataframe for analysis\n",
        "model_df['GMM_Probabilities'] = list(probs)\n",
        "model_df['GMM_Predictions'] = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH1A-zU6ARTy"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for the new data\n",
        "data = {\n",
        "    'Latitude': lats_np_loaded.flatten(),\n",
        "    'Longitude': longs_np_loaded.flatten(),\n",
        "    'ERC': erc_np_loaded.flatten(),\n",
        "    'WUI proximity': wui_np_loaded.flatten(),\n",
        "    'BI': bi_np_loaded.flatten(),\n",
        "    'Pyrome': pyrome_np_loaded.flatten(),\n",
        "    'Acres': 0,\n",
        "    'Ignition date': pd.to_datetime('2022-07-01')  # Random date for testing\n",
        "}\n",
        "df_new = pd.DataFrame(data)\n",
        "df_new = df_new.dropna()\n",
        "\n",
        "\n",
        "# Convert ERC, WUI proximity, and BI to percentiles based on the original data\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df_new = convert_to_percentile_based_on_original(df_new, df, col, 'Pyrome')\n",
        "\n",
        "# Compute the ignition date as cyclically encoded variables\n",
        "df_new['ignition_sin'] = df_new['Ignition date'].apply(date_sin)\n",
        "df_new['ignition_cos'] = df_new['Ignition date'].apply(date_cos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2QRg4TMASGx"
      },
      "outputs": [],
      "source": [
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3s0yGHQ7yI0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def convert_to_percentile_based_on_original(df_new, df_original, column, group_by_column):\n",
        "    percentiles = df_original.groupby(group_by_column)[column].rank(pct=True) * 100\n",
        "    df_new[f'{column}_percentile'] = np.interp(df_new[column], np.sort(df_original[column]), percentiles)\n",
        "    return df_new\n",
        "\n",
        "# Load the numpy arrays\n",
        "bi_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/bi_np.pkl')\n",
        "erc_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/erc_np.pkl')\n",
        "pyrome_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/pyrome_np.pkl')\n",
        "wui_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/wui_np.pkl')\n",
        "lats_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/lats_np.pkl', nodata_value=None)\n",
        "longs_np_loaded = load_np_array('/content/drive/My Drive/CloudFire/data/vars_50k/longs_np.pkl', nodata_value=None)\n",
        "\n",
        "# Create a DataFrame for the new data\n",
        "data = {\n",
        "    'Latitude': lats_np_loaded.flatten(),\n",
        "    'Longitude': longs_np_loaded.flatten(),\n",
        "    'ERC': erc_np_loaded.flatten(),\n",
        "    'WUI proximity': wui_np_loaded.flatten(),\n",
        "    'BI': bi_np_loaded.flatten(),\n",
        "    'Pyrome': pyrome_np_loaded.flatten(),\n",
        "    'Acres': 0,\n",
        "    'Ignition date': pd.to_datetime('2022-07-01'),  # Random date for testing\n",
        "    'Index': np.arange(lats_np_loaded.size),  # Track original indices\n",
        "    'x': np.tile(np.arange(lats_np_loaded.shape[1]), lats_np_loaded.shape[0]),\n",
        "    'y': np.repeat(np.arange(lats_np_loaded.shape[0]), lats_np_loaded.shape[1])\n",
        "}\n",
        "df_new = pd.DataFrame(data)\n",
        "df_new = df_new.dropna()\n",
        "\n",
        "\n",
        "# Convert ERC, WUI proximity, and BI to percentiles based on the original data\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df_new = convert_to_percentile_based_on_original(df_new, df, col, 'Pyrome')\n",
        "\n",
        "# Compute the ignition date as cyclically encoded variables\n",
        "df_new['ignition_sin'] = df_new['Ignition date'].apply(date_sin)\n",
        "df_new['ignition_cos'] = df_new['Ignition date'].apply(date_cos)\n",
        "\n",
        "print(df_new.head())\n",
        "# One-hot encode the pyrome column\n",
        "pyrome_encoded_new = encoder.transform(df_new[['Pyrome']]).toarray()\n",
        "pyrome_df_new = pd.DataFrame(pyrome_encoded_new, columns=encoder.get_feature_names_out(['Pyrome']))\n",
        "\n",
        "\n",
        "# Combine all features into a single dataframe\n",
        "# Combine all features into a single dataframe\n",
        "df_new = pd.concat([df_new[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos', 'Index', 'x', 'y']], pyrome_df_new], axis=1)\n",
        "\n",
        "df_new = df_new.dropna()\n",
        "\n",
        "# Standardize continuous variables using the original scaler\n",
        "continuous_vars_new = df_new[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos']]\n",
        "scaled_vars_new = scaler.transform(continuous_vars_new)\n",
        "scaled_df_new = pd.DataFrame(scaled_vars_new, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "df_new_scaled = pd.concat([scaled_df_new, df_new[encoder.get_feature_names_out(['Pyrome'])], df_new[['Index', 'x', 'y']]], axis=1)\n",
        "\n",
        "print(df_new_scaled.shape)\n",
        "df_new_scaled = df_new_scaled.dropna()\n",
        "print(df_new_scaled.shape)\n",
        "# Run predictions through the GMM\n",
        "X_new = df_new_scaled.drop(columns=['Index', 'x', 'y']).values\n",
        "probs = optimal_gmm.predict_proba(X_new)\n",
        "predictions = optimal_gmm.predict(X_new)\n",
        "\n",
        "# Append the probabilities and predictions to the dataframe for analysis\n",
        "df_new_scaled['GMM_Probabilities'] = list(probs)\n",
        "df_new_scaled['GMM_Predictions'] = predictions\n",
        "\n",
        "print(df_new_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMi0oJnBKSjp"
      },
      "outputs": [],
      "source": [
        "df_new_scaled.head(-40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ-4FPx2Gnd1"
      },
      "outputs": [],
      "source": [
        "# Ensure X_new is correctly formatted and scaled\n",
        "continuous_vars_new = df_new[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos']]\n",
        "scaled_vars_new = scaler.transform(continuous_vars_new)\n",
        "scaled_df_new = pd.DataFrame(scaled_vars_new, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'ignition_sin', 'ignition_cos'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "df_new_scaled = pd.concat([scaled_df_new, df_new[encoder.get_feature_names_out(['Pyrome'])], df_new[['Index', 'x', 'y']]], axis=1)\n",
        "\n",
        "# Drop any remaining NaN values\n",
        "df_new_scaled = df_new_scaled.dropna()\n",
        "\n",
        "# Prepare X_new for GMM prediction\n",
        "X_new = df_new_scaled.drop(columns=['Index', 'x', 'y']).values\n",
        "\n",
        "# Run predictions through the GMM\n",
        "probs = optimal_gmm.predict_proba(X_new)\n",
        "predictions = optimal_gmm.predict(X_new)\n",
        "\n",
        "# Initialize arrays to hold the results\n",
        "probs_reshaped = np.full((36, 46, probs.shape[1]), np.nan)\n",
        "predictions_reshaped = np.full(lats_np_loaded.shape, np.nan)\n",
        "print(probs_reshaped.shape)\n",
        "print(predictions_reshaped.shape)\n",
        "print(df_new_scaled.shape)\n",
        "# Reshape predictions to match the original 2D grid format\n",
        "# Place the predictions back into the original 2D grid format\n",
        "for i, row in df_new_scaled.iterrows():\n",
        "    x = int(row['x'])\n",
        "    y = int(row['y'])\n",
        "    #print(i, x, y)\n",
        "    # if i > 968:\n",
        "    #   print(i, x, y)\n",
        "    #   continue\n",
        "    probs_reshaped[y, x, :] = probs[i]\n",
        "    predictions_reshaped[y, x] = predictions[i]\n",
        "\n",
        "# Display the shapes of the prediction arrays\n",
        "print(\"Shapes of the prediction arrays:\")\n",
        "print(f\"Probs: {probs_reshaped.shape}\")\n",
        "print(f\"Predictions: {predictions_reshaped.shape}\")\n",
        "\n",
        "# Create a figure with two subplots for predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot GMM Component Probabilities (First Component as Example)\n",
        "axs[0].imshow(probs_reshaped[..., 0], cmap='viridis')\n",
        "axs[0].set_title('GMM Component Probabilities (Component 1)')\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Plot GMM Predictions\n",
        "axs[1].imshow(predictions_reshaped, cmap='viridis')\n",
        "axs[1].set_title('GMM Predictions')\n",
        "axs[1].axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Function to sample from the GMM to predict fire size\n",
        "def predict_fire_size(gmm, X, num_samples=1):\n",
        "    means = gmm.means_\n",
        "    covariances = gmm.covariances_\n",
        "    weights = gmm.weights_\n",
        "\n",
        "    # Assuming the last dimension corresponds to fire size\n",
        "    fire_size_samples = np.zeros((X.shape[0], num_samples))\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        component = np.random.choice(len(weights), p=gmm.predict_proba(X[i].reshape(1, -1))[0])\n",
        "        mean = means[component, -1]\n",
        "        covariance = covariances[component, -1, -1]\n",
        "\n",
        "        fire_size_samples[i, :] = np.random.normal(mean, np.sqrt(covariance), num_samples)\n",
        "\n",
        "    return fire_size_samples\n",
        "\n",
        "# Predict fire size for the new data\n",
        "fire_size_predictions = predict_fire_size(optimal_gmm, X_new, num_samples=1)\n",
        "\n",
        "# Reshape fire size predictions to match the original 2D grid format\n",
        "fire_size_predictions_reshaped = np.full(lats_np_loaded.shape, np.nan)\n",
        "\n",
        "for i, row in df_new_scaled.iterrows():\n",
        "    x = int(row['x'])\n",
        "    y = int(row['y'])\n",
        "    fire_size_predictions_reshaped[y, x] = fire_size_predictions[i]\n",
        "\n",
        "# Plot the fire size predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(fire_size_predictions_reshaped, cmap='viridis')\n",
        "plt.title('Predicted Fire Sizes')\n",
        "plt.colorbar(label='Fire Size (Acres)')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fuApJoZGseB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oWOexxvEDRz"
      },
      "outputs": [],
      "source": [
        "probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtJMtdnYDqz6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reshape predictions to match the original data shape\n",
        "probs_reshaped = probs.reshape(lats_np_loaded.shape + (-1,))\n",
        "predictions_reshaped = predictions.reshape(lats_np_loaded.shape)\n",
        "\n",
        "# Display the shapes of the prediction arrays\n",
        "print(\"Shapes of the prediction arrays:\")\n",
        "print(f\"Probs: {probs_reshaped.shape}\")\n",
        "print(f\"Predictions: {predictions_reshaped.shape}\")\n",
        "\n",
        "# Create a figure with two subplots for predictions\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot GMM Component Probabilities (First Component as Example)\n",
        "axs[0].imshow(probs_reshaped[..., 0], cmap='viridis')\n",
        "axs[0].set_title('GMM Component Probabilities (Component 1)')\n",
        "axs[0].axis('off')\n",
        "\n",
        "# Plot GMM Predictions\n",
        "axs[1].imshow(predictions_reshaped, cmap='viridis')\n",
        "axs[1].set_title('GMM Predictions')\n",
        "axs[1].axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaPVw8OJDVIG"
      },
      "outputs": [],
      "source": [
        "df_new_scaled.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYoqU1I9DeK"
      },
      "source": [
        "## GMM PLots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOwZXKiV00gN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Function to select the optimal number of components using BIC\n",
        "def select_optimal_gmm(X, max_components=10):\n",
        "    bic_scores = []\n",
        "    models = []\n",
        "\n",
        "    for n_components in range(1, max_components + 10):\n",
        "        model = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "        model.fit(X)\n",
        "        bic = model.bic(X)\n",
        "        bic_scores.append(bic)\n",
        "        models.append(model)\n",
        "\n",
        "    optimal_index = np.argmin(bic_scores)\n",
        "    optimal_model = models[optimal_index]\n",
        "\n",
        "    return optimal_model, bic_scores, optimal_index + 1\n",
        "\n",
        "# Extract the target variable (fire sizes)\n",
        "y = model_df['Acres'].values.reshape(-1, 1)\n",
        "\n",
        "# Select the optimal number of components\n",
        "optimal_gmm, bic_scores, optimal_n_components = select_optimal_gmm(y)\n",
        "\n",
        "print(f\"Optimal number of components: {optimal_n_components}\")\n",
        "\n",
        "# Fit the GMM with the optimal number of components\n",
        "optimal_gmm.fit(y)\n",
        "\n",
        "# Function to plot the PDF of the GMM\n",
        "def plot_gmm_pdf(gmm, y, n_bins=50):\n",
        "    # Create a range of values for fire sizes\n",
        "    x = np.linspace(y.min(), y.max(), 1000)\n",
        "\n",
        "    # Calculate the PDF for each component\n",
        "    pdf = np.zeros_like(x)\n",
        "    for mean, covariance, weight in zip(gmm.means_.flatten(), gmm.covariances_.flatten(), gmm.weights_):\n",
        "        pdf += weight * norm.pdf(x, mean, np.sqrt(covariance))\n",
        "\n",
        "    # Plot the PDF\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(y, bins=n_bins, density=True, alpha=0.6, color='g', label='Actual Fire Sizes')\n",
        "    plt.plot(x, pdf, color='red', label='GMM PDF')\n",
        "    plt.axvline(np.percentile(y, 75), color='blue', linestyle='--', label='75th Percentile')\n",
        "    plt.xlabel('Fire Size (Acres)')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Probability Density Function of Fire Sizes')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to analyze the frequency of GMM predictions\n",
        "def analyze_gmm_predictions(gmm, y):\n",
        "    # Predict the component probabilities for each sample\n",
        "    probs = gmm.predict_proba(y)\n",
        "\n",
        "    # Predict the component means for each sample\n",
        "    predictions = gmm.predict(y)\n",
        "\n",
        "    # Append the probabilities and predictions to the dataframe for analysis\n",
        "    model_df['GMM_Probabilities'] = list(probs)\n",
        "    model_df['GMM_Predictions'] = predictions\n",
        "\n",
        "    # Calculate the frequency of predictions in the upper quartile\n",
        "    upper_quartile_threshold = np.percentile(y, 75)\n",
        "    upper_quartile_predictions = model_df[model_df['Acres'] >= upper_quartile_threshold]['GMM_Predictions'].value_counts(normalize=True)\n",
        "\n",
        "    return upper_quartile_predictions\n",
        "\n",
        "# Plot the PDF of the GMM\n",
        "plot_gmm_pdf(optimal_gmm, y)\n",
        "\n",
        "# Analyze the frequency of GMM predictions in the upper quartile\n",
        "upper_quartile_freq = analyze_gmm_predictions(optimal_gmm, y)\n",
        "\n",
        "print(\"Frequency of GMM predictions in the upper quartile of fire sizes:\")\n",
        "print(upper_quartile_freq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FexdYZ61xYi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the component indices and their proportions\n",
        "components = upper_quartile_freq.index\n",
        "proportions = upper_quartile_freq.values\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(components, proportions, color='skyblue')\n",
        "plt.xlabel('GMM Component')\n",
        "plt.ylabel('Proportion in Upper Quartile')\n",
        "plt.title('Proportion of GMM Components in the Upper Quartile of Fire Sizes')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwMkqy3YL72s"
      },
      "outputs": [],
      "source": [
        "# Evaluate the log-likelihood\n",
        "log_likelihood = optimal_gmm.score(X)\n",
        "print(f\"Log-likelihood of the model: {log_likelihood}\")\n",
        "\n",
        "# Visualize the distribution of predicted fire sizes for different components\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot BIC scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(bic_scores) + 1), bic_scores, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('BIC Score')\n",
        "plt.title('BIC Scores for Different Number of Components')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the GMM components and fire size distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(optimal_n_components):\n",
        "    component_data = model_df[model_df['GMM_Predictions'] == i]\n",
        "    plt.hist(component_data['Acres'], bins=30, alpha=0.5, label=f'Component {i + 1}')\n",
        "plt.xlabel('Fire Size (Acres)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Fire Size Distribution by GMM Components')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQZMhYMaNOkh"
      },
      "outputs": [],
      "source": [
        "# Frequency of Fire Sizes by GMM Components (Log-scaled)\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(optimal_n_components):\n",
        "    component_data = model_df[model_df['GMM_Predictions'] == i]\n",
        "    plt.hist(component_data['Acres'], bins=30, alpha=0.6, label=f'Component {i + 1}', log=True)\n",
        "plt.xlabel('Fire Size (Acres)')\n",
        "plt.ylabel('Frequency (Log-scaled)')\n",
        "plt.title('Log-scaled Frequency of Fire Sizes by GMM Components')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1BVNXi-twV4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Function to select the optimal number of components using BIC\n",
        "def select_optimal_gmm(X, max_components=10):\n",
        "    bic_scores = []\n",
        "    models = []\n",
        "\n",
        "    for n_components in range(1, max_components + 1):\n",
        "        model = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "        model.fit(X)\n",
        "        bic = model.bic(X)\n",
        "        bic_scores.append(bic)\n",
        "        models.append(model)\n",
        "\n",
        "    optimal_index = np.argmin(bic_scores)\n",
        "    optimal_model = models[optimal_index]\n",
        "\n",
        "    return optimal_model, bic_scores, optimal_index + 1\n",
        "\n",
        "# Extract features and target variable\n",
        "X = model_df.drop(columns=['Acres']).values\n",
        "y = model_df['Acres'].values\n",
        "\n",
        "# Select the optimal number of components\n",
        "optimal_gmm, bic_scores, optimal_n_components = select_optimal_gmm(X)\n",
        "\n",
        "print(f\"Optimal number of components: {optimal_n_components}\")\n",
        "\n",
        "# Fit the GMM with the optimal number of components\n",
        "optimal_gmm.fit(X)\n",
        "\n",
        "# Predict the component probabilities for each sample\n",
        "probs = optimal_gmm.predict_proba(X)\n",
        "\n",
        "# Predict the component means for each sample\n",
        "predictions = optimal_gmm.predict(X)\n",
        "\n",
        "# Append the probabilities and predictions to the dataframe for analysis\n",
        "model_df['GMM_Probabilities'] = list(probs)\n",
        "model_df['GMM_Predictions'] = predictions\n",
        "\n",
        "# Extract the means and covariances of the components\n",
        "means = optimal_gmm.means_\n",
        "covariances = optimal_gmm.covariances_\n",
        "\n",
        "# Print the means and covariances to debug\n",
        "print(\"Component Means:\")\n",
        "print(means)\n",
        "\n",
        "print(\"Component Covariances:\")\n",
        "print(covariances)\n",
        "\n",
        "# Generate the PDF for fire sizes\n",
        "x_values = np.linspace(min(y), max(y), 1000)\n",
        "pdf_values = np.zeros_like(x_values)\n",
        "\n",
        "# Print range of x_values for debugging\n",
        "print(\"x_values range:\")\n",
        "print(x_values[:10], \"to\", x_values[-10:])\n",
        "\n",
        "for mean, cov in zip(means, covariances):\n",
        "    component_pdf = norm.pdf(x_values, mean[-1], np.sqrt(cov[-1, -1]))\n",
        "    pdf_values += component_pdf\n",
        "\n",
        "    # Print intermediate pdf values for debugging\n",
        "    print(\"Intermediate component PDF values:\")\n",
        "    print(component_pdf[:10])\n",
        "\n",
        "# Normalize the PDF\n",
        "pdf_sum = pdf_values.sum()\n",
        "if pdf_sum > 0:\n",
        "    pdf_values /= pdf_sum\n",
        "else:\n",
        "    print(\"Warning: Sum of PDF values is zero or negative, indicating an issue.\")\n",
        "\n",
        "# Print final PDF values for debugging\n",
        "print(\"Final PDF values (first 10):\")\n",
        "print(pdf_values[:10])\n",
        "\n",
        "# Calculate the upper quartile threshold\n",
        "upper_quartile_threshold = np.percentile(y, 75)\n",
        "\n",
        "# Print upper quartile threshold for debugging\n",
        "print(\"Upper Quartile Threshold:\")\n",
        "print(upper_quartile_threshold)\n",
        "\n",
        "# Plot the PDF for fire sizes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, pdf_values, label='PDF of Fire Sizes')\n",
        "plt.axvline(x=upper_quartile_threshold, color='r', linestyle='--', label='75th Percentile (Upper Quartile)')\n",
        "plt.xlabel('Fire Size (Acres)')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Probability Density Function (PDF) of Fire Sizes')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWXrVZ3NR6K"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "\n",
        "# This function generates continuous heatmaps for each GMM component, showing how the component's weights\n",
        "# (responsibilities) are distributed across BI and ERC percentiles. The process involves:\n",
        "# 1. Generating a grid of points over the range of BI and ERC percentiles.\n",
        "# 2. Extracting the weights (responsibilities) for each GMM component from the dataframe.\n",
        "# 3. Fitting a Kernel Density Estimation (KDE) using BI and ERC percentiles, weighted by the component's responsibilities.\n",
        "# 4. Evaluating the KDE on the grid points to create a smooth density estimate.\n",
        "# 5. Plotting the density estimate as a heatmap for each component, ensuring all plots are square and using a consistent color scale.\n",
        "\n",
        "# This visualization helps in understanding how the GMM partitions the data across BI and ERC percentiles, highlighting\n",
        "# the regions where each component has higher weights.\n",
        "\n",
        "\n",
        "def plot_gmm_component_heatmaps(df, n_components, x_col, y_col, weights_col, grid_size=100):\n",
        "    # Generate grid for plotting\n",
        "    x_min, x_max = df[x_col].min(), df[x_col].max()\n",
        "    y_min, y_max = df[y_col].min(), df[y_col].max()\n",
        "    x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, grid_size), np.linspace(y_min, y_max, grid_size))\n",
        "    grid_points = np.vstack([x_grid.ravel(), y_grid.ravel()]).T\n",
        "\n",
        "    # Initialize list to store KDE values for scaling color bars\n",
        "    kde_values = []\n",
        "\n",
        "    for i in range(n_components):\n",
        "        # Extract weights for the current component\n",
        "        weights = df[weights_col].apply(lambda x: x[i]).values\n",
        "\n",
        "        # Fit KDE for the current component\n",
        "        kde = gaussian_kde(df[[x_col, y_col]].T, weights=weights)\n",
        "\n",
        "        # Evaluate KDE on the grid\n",
        "        z = kde(grid_points.T).reshape(x_grid.shape)\n",
        "        kde_values.append(z)\n",
        "\n",
        "    # Determine the global minimum and maximum KDE values for consistent color bars\n",
        "    z_min = min(z.min() for z in kde_values)\n",
        "    z_max = max(z.max() for z in kde_values)\n",
        "\n",
        "    plt.figure(figsize=(22, 22))\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    for i in range(n_components):\n",
        "      # ValueError: Number of rows must be a positive integer, not 4.0\n",
        "        n_rows = int(np.ceil(n_components / 2))\n",
        "\n",
        "        plt.subplot(n_rows, 2, i + 1)\n",
        "\n",
        "        # Plot the heatmap with consistent color scale\n",
        "        z = kde_values[i]\n",
        "        contour = plt.contourf(x_grid, y_grid, z, levels=20, cmap='viridis', vmin=z_min, vmax=z_max)\n",
        "        plt.colorbar(contour, label='Density')\n",
        "        plt.xlabel(x_col)\n",
        "        plt.ylabel(y_col)\n",
        "        plt.title(f'GMM Component {i + 1} Weighting')\n",
        "        plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_gmm_component_heatmaps(model_df, optimal_n_components, 'BI_percentile', 'ERC_percentile', 'GMM_Probabilities')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-jDQWCMNlWE"
      },
      "outputs": [],
      "source": [
        "# Heatmap of the Correlation Matrix for Each GMM Component\n",
        "for i in range(optimal_n_components):\n",
        "    component_data = model_df[model_df['GMM_Predictions'] == i]\n",
        "    correlation_matrix = component_data[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres']].corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title(f'Correlation Matrix for GMM Component {i + 1}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAdbxraWNXZG"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Box Plot of Fire Sizes by GMM Components\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='GMM_Predictions', y='Acres', data=model_df)\n",
        "plt.xlabel('GMM Component')\n",
        "plt.ylabel('Fire Size (Acres)')\n",
        "plt.yscale('log')\n",
        "plt.title('Box Plot of Fire Sizes by GMM Components (Log-scaled)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUt2f1VATVfB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Fit the GMM (assuming optimal_gmm is already fitted with the best number of components)\n",
        "# If not already done:\n",
        "# optimal_gmm = GaussianMixture(n_components=optimal_n_components, covariance_type='full', random_state=42)\n",
        "# optimal_gmm.fit(X)\n",
        "\n",
        "# Generate a range of fire sizes to evaluate the PDF\n",
        "fire_size_range = np.linspace(model_df['Acres'].min(), model_df['Acres'].max(), 1000)\n",
        "\n",
        "# Evaluate the PDF for each fire size using the GMM\n",
        "log_prob = optimal_gmm.score_samples(fire_size_range.reshape(-1, 1))\n",
        "pdf = np.exp(log_prob)\n",
        "\n",
        "# Plot the PDF\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fire_size_range, pdf, label='GMM PDF')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Fire Size (Acres)')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.title('Probability Density Function of Fire Sizes under the GMM')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li_cJVDyOW3_"
      },
      "outputs": [],
      "source": [
        "# Histogram of Fire Sizes for Large Fires (Log-scaled)\n",
        "large_fires = model_df[model_df['Acres'] > 50000]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(large_fires['Acres'], bins=30, alpha=0.7, color='red', log=True)\n",
        "plt.xlabel('Fire Size (Acres)')\n",
        "plt.ylabel('Frequency (Log-scaled)')\n",
        "plt.title('Log-scaled Frequency of Large Fire Sizes (> 50,000 Acres)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-F1OCbmtCX"
      },
      "source": [
        "# Learning PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9P4We92oAoO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "df['Ignition date'] = pd.to_datetime(df['Ignition date'])\n",
        "df['Containment date'] = pd.to_datetime(df['Containment date'])\n",
        "\n",
        "# Calculate fire duration\n",
        "df['Duration'] = (df['Containment date'] - df['Ignition date']).dt.days\n",
        "df = df[['Latitude', 'Longitude', 'ERC', 'WUI proximity', 'BI', 'Pyrome', 'Acres']]\n",
        "\n",
        "# Filter out the dataset to include only the Western United States\n",
        "df = df[df['Pyrome'] <60]\n",
        "\n",
        "# Display the shape of the filtered dataset to confirm the filtering\n",
        "print(df.shape)\n",
        "\n",
        "# Handle missing values (example: fill with mean)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Convert ERC, WUI, and BI to percentiles within each pyrome\n",
        "def convert_to_percentile(df, column, group_by_column):\n",
        "    df[f'{column}_percentile'] = df.groupby(group_by_column)[column].rank(pct=True) * 100\n",
        "    return df\n",
        "\n",
        "for col in ['ERC', 'WUI proximity', 'BI']:\n",
        "    df = convert_to_percentile(df, col, 'Pyrome')\n",
        "\n",
        "#df = df[df['Pyrome'] == 34]\n",
        "print(df.shape)\n",
        "# One-hot encode the pyrome column\n",
        "encoder = OneHotEncoder()\n",
        "pyrome_encoded = encoder.fit_transform(df[['Pyrome']]).toarray()\n",
        "pyrome_df = pd.DataFrame(pyrome_encoded, columns=encoder.get_feature_names_out(['Pyrome']))\n",
        "\n",
        "# Combine all features into a single dataframe\n",
        "model_df = pd.concat([df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres']], pyrome_df], axis=1)\n",
        "\n",
        "# Standardize continuous variables\n",
        "scaler = StandardScaler()\n",
        "continuous_vars = model_df[['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile']]\n",
        "scaled_vars = scaler.fit_transform(continuous_vars)\n",
        "scaled_df = pd.DataFrame(scaled_vars, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile'])\n",
        "\n",
        "# Combine scaled continuous variables and categorical variables\n",
        "model_df = pd.concat([scaled_df, model_df[encoder.get_feature_names_out(['Pyrome'])], model_df['Acres']], axis=1)\n",
        "\n",
        "# Filter for Pyrome_34 = 1\n",
        "#model_df = model_df[model_df['Pyrome_34'] == 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYn5ZOZ9qo2J"
      },
      "outputs": [],
      "source": [
        "model_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H66nrJV2qOAh"
      },
      "outputs": [],
      "source": [
        "# Extract features and target\n",
        "X = model_df.drop('Acres', axis=1).values\n",
        "y = model_df['Acres'].values.reshape(-1, 1)\n",
        "\n",
        "# Combine features and target\n",
        "data = np.hstack((X, y))\n",
        "\n",
        "# Fit a KDE model\n",
        "kde = KernelDensity(kernel='gaussian', bandwidth=0.5)\n",
        "kde.fit(data)\n",
        "\n",
        "# Function to sample from the KDE\n",
        "def sample_kde(kde, num_samples):\n",
        "    samples = kde.sample(num_samples)\n",
        "    return samples\n",
        "\n",
        "# Generate samples from the KDE\n",
        "num_samples = 100000\n",
        "samples = sample_kde(kde, num_samples)\n",
        "\n",
        "# Separate the sampled features and target\n",
        "sampled_features = samples[:, :-1]\n",
        "sampled_acres = samples[:, -1]\n",
        "\n",
        "# Reverse the standardization for continuous variables\n",
        "scaled_features = scaler.inverse_transform(sampled_features[:, :5])\n",
        "sampled_lat_long_erc_wui_bi = pd.DataFrame(scaled_features, columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile'])\n",
        "\n",
        "# Decode pyrome categories\n",
        "sampled_pyrome = encoder.inverse_transform(sampled_features[:, 5:])\n",
        "\n",
        "# Combine all sampled data into a dataframe\n",
        "sampled_df = pd.DataFrame(np.hstack((sampled_lat_long_erc_wui_bi, sampled_pyrome, sampled_acres.reshape(-1, 1))), columns=['Latitude', 'Longitude', 'ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Pyrome', 'Acres'])\n",
        "\n",
        "# Display the first few rows of the sampled dataframe\n",
        "sampled_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBCffjPe3VJk"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ozGdMRaFxJ2s"
      },
      "outputs": [],
      "source": [
        "# Create pair plots for the original and sampled data\n",
        "def plot_pairplots(original_df, sampled_df, variables, pyrome):\n",
        "    original_subset = original_df[original_df['Pyrome'] == pyrome][variables]\n",
        "    sampled_subset = sampled_df[sampled_df['Pyrome'] == pyrome][variables]\n",
        "\n",
        "    # Add a 'Dataset' column to distinguish between original and sampled data\n",
        "    original_subset['Dataset'] = 'Original'\n",
        "    sampled_subset['Dataset'] = 'Sampled'\n",
        "\n",
        "    combined_df = pd.concat([original_subset, sampled_subset])\n",
        "\n",
        "    sns.pairplot(combined_df, hue='Dataset', corner=True, plot_kws={'alpha': 0.5})\n",
        "    plt.suptitle(f'Pair Plot for Pyrome {pyrome}', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# Plot pair plots for key variables and each pyrome\n",
        "variables = ['ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres']\n",
        "for pyrome in df['Pyrome'].unique():\n",
        "    plot_pairplots(df, sampled_df, variables, pyrome)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8hqAcKlxj3D"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot KDE comparison for each pyrome\n",
        "def plot_kde_comparison(original_df, sampled_df, variable, pyrome_column='Pyrome'):\n",
        "    pyromes = original_df[pyrome_column].unique()\n",
        "    for pyrome in pyromes:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "\n",
        "        # Plot original data KDE\n",
        "        sns.kdeplot(data=original_df[original_df[pyrome_column] == pyrome], x=variable, label='Original Data', fill=True)\n",
        "\n",
        "        # Plot sampled data KDE\n",
        "        sns.kdeplot(data=sampled_df[sampled_df[pyrome_column] == pyrome], x=variable, label='Sampled Data', fill=True)\n",
        "\n",
        "        plt.title(f'KDE Plot of {variable} for Pyrome {pyrome}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Convert the sampled_df 'Pyrome' back to categorical for proper comparison\n",
        "sampled_df['Pyrome'] = sampled_df['Pyrome'].astype(int).astype(str)\n",
        "\n",
        "# Plot KDE comparison for ERC, WUI proximity, BI, and Acres\n",
        "for var in ['ERC_percentile', 'WUI proximity_percentile', 'BI_percentile', 'Acres']:\n",
        "    plot_kde_comparison(df, sampled_df, var)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4EYoqU1I9DeK"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0da3490dec0a4f179b24c93db728020c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ff0428713fe40ab9d87ac81c688f7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a24e21b0d4647d1abd12ef9be82e81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "500bfeb6a5f443d68ab635216977f0b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a22c1e1cd24b3c8f620bf46801df6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a90d4b38ce854185a4c960a29bec9994",
              "IPY_MODEL_b519f0219a604bbf8fde3fc336f394c8"
            ],
            "layout": "IPY_MODEL_e5d5f972765044c7943a41040ffa5f8a"
          }
        },
        "6f6715179a8345ecacefd4602ade22a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fa121395b9a44c58ec260902a0f0429": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "733fdc4275bc4ef49bb4378b60744ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8338fd748e904b239f8d4271f9d9d42f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90d4b38ce854185a4c960a29bec9994": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fa121395b9a44c58ec260902a0f0429",
            "placeholder": "​",
            "style": "IPY_MODEL_3a24e21b0d4647d1abd12ef9be82e81c",
            "value": "7.806 MB of 7.806 MB uploaded\r"
          }
        },
        "b519f0219a604bbf8fde3fc336f394c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_500bfeb6a5f443d68ab635216977f0b1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f6715179a8345ecacefd4602ade22a9",
            "value": 1
          }
        },
        "b83b7023cb1545418fe640c6432f6b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be7ef8d010a742d29590f470df8cc00f",
              "IPY_MODEL_c121991403bf4428a334e0437ef75178"
            ],
            "layout": "IPY_MODEL_ec49634d24bc4417b5252e3a512816fe"
          }
        },
        "be7ef8d010a742d29590f470df8cc00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_733fdc4275bc4ef49bb4378b60744ccb",
            "placeholder": "​",
            "style": "IPY_MODEL_0da3490dec0a4f179b24c93db728020c",
            "value": "54.868 MB of 54.868 MB uploaded\r"
          }
        },
        "c121991403bf4428a334e0437ef75178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8338fd748e904b239f8d4271f9d9d42f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ff0428713fe40ab9d87ac81c688f7b4",
            "value": 1
          }
        },
        "e5d5f972765044c7943a41040ffa5f8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec49634d24bc4417b5252e3a512816fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
